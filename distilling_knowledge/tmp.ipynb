{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ds/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:absl:GlobalAsyncCheckpointManager is not imported correctly. Checkpointing of GlobalDeviceArrays will not be available.To use the feature, install tensorstore.\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import flax.linen as nn\n",
    "import jax.numpy as jnp\n",
    "from typing import Sequence\n",
    "import optax\n",
    "import numpy as np\n",
    "from flax.training import train_state, checkpoints, early_stopping\n",
    "from flax.metrics import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307), (0.3081)),\n",
    "    transforms.Lambda(lambda x: torch.flatten(x))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    features: Sequence[int]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        for feat in self.features[:-1]:\n",
    "            x = nn.relu(nn.Dense(feat)(x))\n",
    "        x = nn.Dense(self.features[-1])(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def apply_model(state, data, labels):\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({'params': params}, data)\n",
    "        one_hot = jax.nn.one_hot(labels, 10)\n",
    "        loss = jnp.mean(optax.softmax_cross_entropy(logits=logits, labels=one_hot))\n",
    "        return loss, logits\n",
    "    \n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, logits), grads = grad_fn(state.params)\n",
    "    accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "    return grads, loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(output, y):\n",
    "    return -jnp.mean(y * output - (1.0 - y) * (jnp.exp(output - 1.0)), axis=0)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def apply_student_model(state, data, labels):\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({'params': params}, data)\n",
    "        one_hot = jax.nn.one_hot(labels, 10)\n",
    "        loss = 0.5 * jnp.mean(optax.softmax_cross_entropy(logits=logits, labels=one_hot)) + 0.5 * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update_model(state, grads): \n",
    "    return state.apply_gradients(grads=grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(state, train_dt, rng):\n",
    "    epoch_loss = []\n",
    "    epoch_accuracy = []\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_dt):\n",
    "        data, target = data.numpy(), target.numpy()\n",
    "        data, target = jnp.float32(data), jnp.float32(target)\n",
    "\n",
    "        grads, loss, accuracy = apply_model(state, data, target)\n",
    "        state = update_model(state, grads)\n",
    "        epoch_loss.append(loss)\n",
    "        epoch_accuracy.append(accuracy)\n",
    "\n",
    "    train_loss = np.mean(epoch_loss)\n",
    "    train_accuracy = np.mean(epoch_accuracy)\n",
    "\n",
    "    return state, train_loss, train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_state(rng, config):\n",
    "    mlp = MLP([1200, 1200, 10])\n",
    "    params = mlp.init(rng, jnp.ones([1, 784]))['params']\n",
    "    tx = optax.adam(learning_rate)\n",
    "    return train_state.TrainState.create(\n",
    "        apply_fn=mlp.apply, params=params, tx=tx\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(config, workdir):\n",
    "    train = MNIST(root='data/', train=True, download=True, transform=transform)\n",
    "    test = MNIST(root='data/', train=False, download=True, transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(train, **train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(test, **test_kwargs)\n",
    "\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "\n",
    "    summary_writer = tensorboard.SummaryWriter(workdir)\n",
    "    summary_writer.hparams(dict(config))\n",
    "\n",
    "    rng, init_rng = jax.random.split(rng)\n",
    "    state = create_train_state(init_rng, config)\n",
    "\n",
    "    early_stop = early_stopping.EarlyStopping(min_delta=1e-3, patience=2)\n",
    "    best_score = 0\n",
    "\n",
    "    for epoch in range(1, config['num_epoch'] + 1):\n",
    "        rng, init_rng = jax.random.split(rng)\n",
    "        state, train_loss, train_accuracy = train_epoch(state, train_loader, rng)\n",
    "        _, early_stop = early_stop.update(train_loss)\n",
    "        test_dt, test_labels = next(iter(test_loader))\n",
    "        _, test_loss, test_accuracy = apply_model(state, test_dt.numpy(), test_labels.numpy())\n",
    "        print(\n",
    "        'epoch:% 3d, train_loss: %.4f, train_accuracy: %.2f, test_loss: %.4f, test_accuracy: %.2f'\n",
    "        % (epoch, train_loss, train_accuracy * 100, test_loss,\n",
    "           test_accuracy * 100))\n",
    "        \n",
    "        summary_writer.scalar('train_loss', train_loss, epoch)\n",
    "        summary_writer.scalar('train_accuracy', train_accuracy, epoch)\n",
    "        summary_writer.scalar('test_loss', test_loss, epoch)\n",
    "        summary_writer.scalar('test_accuracy', test_accuracy, epoch)\n",
    "        \n",
    "        if test_accuracy > best_score:\n",
    "            checkpoints.save_checkpoint(ckpt_dir=CKPT_DIR, target=state, overwrite=True, step=epoch, prefix='mlp_1200_', )\n",
    "\n",
    "        if early_stop.should_stop:\n",
    "            print('Met early stopping criteria, breaking...')\n",
    "            break\n",
    "\n",
    "    summary_writer.flush()\n",
    "    return state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'num_epoch': 10}\n",
    "train_kwargs = {'batch_size': 8}\n",
    "test_kwargs = {'batch_size': 120}\n",
    "learning_rate = 1e-3\n",
    "num_epoch = 1\n",
    "CKPT_DIR = 'ckpts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1, train_loss: 0.2349, train_accuracy: 93.36, test_loss: 0.1829, test_accuracy: 93.33\n",
      "epoch:  2, train_loss: 0.1368, train_accuracy: 96.31, test_loss: 0.2224, test_accuracy: 93.33\n",
      "epoch:  3, train_loss: 0.1088, train_accuracy: 97.09, test_loss: 0.0729, test_accuracy: 96.67\n",
      "epoch:  4, train_loss: 0.0941, train_accuracy: 97.57, test_loss: 0.0394, test_accuracy: 99.17\n",
      "epoch:  5, train_loss: 0.0896, train_accuracy: 97.72, test_loss: 0.1100, test_accuracy: 99.17\n",
      "epoch:  6, train_loss: 0.0763, train_accuracy: 98.07, test_loss: 0.0374, test_accuracy: 99.17\n",
      "epoch:  7, train_loss: 0.0762, train_accuracy: 98.14, test_loss: 0.0400, test_accuracy: 97.50\n",
      "epoch:  8, train_loss: 0.0753, train_accuracy: 98.22, test_loss: 0.0477, test_accuracy: 97.50\n",
      "epoch:  9, train_loss: 0.0675, train_accuracy: 98.34, test_loss: 0.0168, test_accuracy: 99.17\n",
      "epoch: 10, train_loss: 0.0653, train_accuracy: 98.49, test_loss: 0.0317, test_accuracy: 98.33\n"
     ]
    }
   ],
   "source": [
    "state = train_and_evaluate(config, 'logs/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train student using distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('ds')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5fa8e7a0e7c7188de72acea4ae1bc222d1770499c4c3d36ce32843ef46b20053"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
